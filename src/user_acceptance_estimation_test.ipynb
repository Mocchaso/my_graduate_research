{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from my_data_preparation_kit import get_all_usertalk_data_from_corpus, normalize # ユーザ発話に対する処理\n",
    "from my_data_preparation_kit import word_table_json_name, synonym_table_json_name, load_json_as_dict # JSONファイル周りの処理\n",
    "from my_feature_vector_creator_kit import get_all_words_in_corpus\n",
    "import time\n",
    "import MeCab\n",
    "\n",
    "svr_model = \"tuned_rbf_svr_model.pkl\"\n",
    "constructed_corpus_path = \"dialogue_corpus.db\"\n",
    "pn_ja_dict_path = \"../pn_ja.dic\"\n",
    "\n",
    "# data_preparation.ipynbで保存したword_tableとsynonym_tableを読み込む\n",
    "w_table = load_json_as_dict(word_table_json_name)       # print(convert_dict_to_json(word_table, 4))   -> {\"u\\...\": [], ... }\n",
    "s_table = load_json_as_dict(synonym_table_json_name) # print(word_table[\"食べる\"])                  -> [235301]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading learned SVR model: about 0.2832016944885254 seconds.\n"
     ]
    }
   ],
   "source": [
    "# 学習(チューニング)済みのSVRのモデルを読み込む\n",
    "t1 = time.time()\n",
    "loaded_model = joblib.load(svr_model)\n",
    "t2 = time.time()\n",
    "print(\"Finished loading learned SVR model: about {} seconds.\".format(t2 - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading pn_ja.dic.\n"
     ]
    }
   ],
   "source": [
    "# 単語極性辞書の読み込み\n",
    "pn_table = {}\n",
    "with open(pn_ja_dict_path) as file:\n",
    "    for line in file:\n",
    "        line = line.split(\":\")\n",
    "        pn_table[line[0]] = float(line[3])\n",
    "print(\"Finished loading pn_ja.dic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished getting all user talks from dialogue_corpus.db: about 0.20914721488952637 seconds.\n",
      "Finished getting all words from dialogue_corpus.db: about 0.8055751323699951 seconds.\n",
      "Finished making all_words-list which removed duplication. (order of the list is kept): about 12.727441787719727 seconds.\n"
     ]
    }
   ],
   "source": [
    "# コーパス内の全てのユーザ発話のデータ：[ talk_data -> tuple, ... ]\n",
    "# テストのためのユーザ発話を読み込む\n",
    "all_usertalk_data = get_all_usertalk_data_from_corpus()\n",
    "# コーパス内の全単語\n",
    "all_words_in_corpus = get_all_words_in_corpus()\n",
    "# コーパス内の全単語の重複無しバージョン（順序を保持している）\n",
    "# 参考サイト：https://note.nkmk.me/python-list-unique-duplicate/\n",
    "tt = time.time()\n",
    "all_words_in_corpus_uniq = sorted(set(all_words_in_corpus), key = all_words_in_corpus.index)\n",
    "tt2 = time.time()\n",
    "print(\"Finished making all_words-list which removed duplication. (order of the list is kept): about {} seconds.\".format(tt2 - tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished making a list about words in corpus and their synonyms: about 0.021014928817749023 seconds.\n"
     ]
    }
   ],
   "source": [
    "# コーパス内の全単語とその同義語を、対応する列にまとめたリスト\n",
    "# 対応する単語IDが無い or 同義語が無い場合は、その単語自体のみを含んだリストが格納される\n",
    "t0 = time.time()\n",
    "columns = []\n",
    "for word in all_words_in_corpus_uniq:\n",
    "    column = [word]\n",
    "    wordids = []\n",
    "    if word in w_table.keys(): # wordがユーザ発話に含まれる単語ならば\n",
    "        wordids = w_table[word]\n",
    "    if wordids != []: # wordに対応する単語IDがあれば\n",
    "        for wordid in wordids:\n",
    "            # wordidに対応する同義語をcolumnに追加\n",
    "            synonyms = s_table[str(wordid)]\n",
    "            column.extend(synonyms)\n",
    "    columns.append(column)\n",
    "t01 = time.time()\n",
    "print(\"Finished making a list about words in corpus and their synonyms: about {} seconds.\".format(t01 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# my_feature_vector_creator_kit.pyのmake_svr_learning_featureと\n",
    "# ほぼ同じ手順で数値表現に直す\n",
    "inputted_talk = all_usertalk_data[8000][1]\n",
    "\n",
    "m = MeCab.Tagger(\"-Owakati\")\n",
    "bow = [0] * len(columns) # 入力されたユーザ発話の特徴ベクトル\n",
    "word_pn_scores = 0 # ユーザ発話の極性スコアの記録\n",
    "words_in_inputted_talk = m.parse(inputted_talk).split() # ユーザ発話に含まれる単語のリスト\n",
    "for word in words_in_inputted_talk:\n",
    "    # word：ユーザ発話に含まれる単語\n",
    "    # columns：コーパス内の全単語とその同義語を、対応する列にまとめたリスト\n",
    "    for col_i, column in enumerate(columns):\n",
    "        if word in column: # wordが、対応する列にまとめておいた単語と同義語のリストに含まれていたら\n",
    "            bow[col_i] = 1\n",
    "\n",
    "    if word in pn_table.keys(): # 1つのユーザ発話内の単語の極性スコアを加算\n",
    "        word_pn_scores += pn_table[word]\n",
    "else:\n",
    "    # 1発話の単語について全て調べ終わったら、発話文の各単語の極性スコアの平均値を算出する\n",
    "    # 小数点以下切り捨て防止のため分子にfloat()、0除算防止のため分母に+1\n",
    "    word_pn_ave = float(word_pn_scores) / (len(words_in_inputted_talk) + 1)\n",
    "    bow.append(word_pn_ave)\n",
    "\n",
    "bow = [bow] # 二次元である学習データに合わせるため"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ユーザ発話 = お腹いっぱいで食べれないよ。, 予測値 = [ 2.04937768]\n"
     ]
    }
   ],
   "source": [
    "# 数値表現に直したユーザ発話を用いて、そのユーザ発話に対する時受諾度合いを推定する\n",
    "true_acceptance = all_usertalk_data[8000][3]\n",
    "\n",
    "print(\"ユーザ発話 = {}, 予測値 = {}\".format(inputted_talk, loaded_model.predict(bow)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.04937767528 2\n"
     ]
    }
   ],
   "source": [
    "result = loaded_model.predict(bow)[0]\n",
    "import numpy as np\n",
    "print(result, int(np.round(result)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

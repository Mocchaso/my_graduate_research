{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from my_data_preparation_kit import normalize # ユーザ発話に対する処理(コーパスからユーザ発話を取得する機能は不要)\n",
    "from my_data_preparation_kit import word_table_json_name, synonym_table_json_name, load_json_as_dict # JSONファイル周りの処理\n",
    "from my_feature_vector_creator_kit import get_all_words_in_corpus\n",
    "import time\n",
    "import MeCab\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "svr_model = \"tuned_rbf_svr_model.pkl\"\n",
    "constructed_corpus_path = \"dialogue_corpus.db\"\n",
    "pn_ja_dict_path = \"../pn_ja.dic\"\n",
    "\n",
    "# data_preparation.ipynbで保存したword_tableとsynonym_tableを読み込む\n",
    "w_table = load_json_as_dict(word_table_json_name)       # print(convert_dict_to_json(word_table, 4))   -> {\"u\\...\": [], ... }\n",
    "s_table = load_json_as_dict(synonym_table_json_name) # print(word_table[\"食べる\"])                  -> [235301]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading learned SVR model: about 0.29519104957580566 seconds.\n"
     ]
    }
   ],
   "source": [
    "# 学習(チューニング)済みのSVRのモデルを読み込む\n",
    "t1 = time.time()\n",
    "loaded_model = joblib.load(svr_model)\n",
    "t2 = time.time()\n",
    "print(\"Finished loading learned SVR model: about {} seconds.\".format(t2 - t1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished loading pn_ja.dic.\n"
     ]
    }
   ],
   "source": [
    "# 単語極性辞書の読み込み\n",
    "pn_table = {}\n",
    "with open(pn_ja_dict_path) as file:\n",
    "    for line in file:\n",
    "        line = line.split(\":\")\n",
    "        pn_table[line[0]] = float(line[3])\n",
    "print(\"Finished loading pn_ja.dic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished getting all words from dialogue_corpus.db: about 1.1478192806243896 seconds.\n",
      "Finished making all_words-list which removed duplication. (order of the list is kept): about 17.739309549331665 seconds.\n"
     ]
    }
   ],
   "source": [
    "# コーパス内の全単語\n",
    "all_words_in_corpus = get_all_words_in_corpus()\n",
    "# コーパス内の全単語の重複無しバージョン（順序を保持している）\n",
    "# 参考サイト：https://note.nkmk.me/python-list-unique-duplicate/\n",
    "tt = time.time()\n",
    "all_words_in_corpus_uniq = sorted(set(all_words_in_corpus), key = all_words_in_corpus.index)\n",
    "tt2 = time.time()\n",
    "print(\"Finished making all_words-list which removed duplication. (order of the list is kept): about {} seconds.\".format(tt2 - tt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished making a list about words in corpus and their synonyms: about 0.058042287826538086 seconds.\n"
     ]
    }
   ],
   "source": [
    "# コーパス内の全単語とその同義語を、対応する列にまとめたリスト\n",
    "# 対応する単語IDが無い or 同義語が無い場合は、その単語自体のみを含んだリストが格納される\n",
    "t0 = time.time()\n",
    "columns = []\n",
    "for word in all_words_in_corpus_uniq:\n",
    "    column = [word]\n",
    "    wordids = []\n",
    "    if word in w_table.keys(): # wordがユーザ発話に含まれる単語ならば\n",
    "        wordids = w_table[word]\n",
    "    if wordids != []: # wordに対応する単語IDがあれば\n",
    "        for wordid in wordids:\n",
    "            # wordidに対応する同義語をcolumnに追加\n",
    "            synonyms = s_table[str(wordid)]\n",
    "            column.extend(synonyms)\n",
    "    columns.append(column)\n",
    "t01 = time.time()\n",
    "print(\"Finished making a list about words in corpus and their synonyms: about {} seconds.\".format(t01 - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# my_feature_vector_creator_kit.pyのmake_svr_learning_featureと\n",
    "# ほぼ同じ手順で数値表現に直す\n",
    "def make_bow_of_inputted_usertalk(inputted_usertalk):\n",
    "    m = MeCab.Tagger(\"-Owakati\")\n",
    "    bow = [0] * len(columns) # 入力されたユーザ発話の特徴ベクトル\n",
    "    word_pn_scores = 0 # ユーザ発話の極性スコアの記録\n",
    "    words_in_inputted_talk = m.parse(inputted_usertalk).split() # ユーザ発話に含まれる単語のリスト\n",
    "    for word in words_in_inputted_talk:\n",
    "        # word：ユーザ発話に含まれる単語\n",
    "        # columns：コーパス内の全単語とその同義語を、対応する列にまとめたリスト\n",
    "        for col_i, column in enumerate(columns):\n",
    "            if word in column: # wordが、対応する列にまとめておいた単語と同義語のリストに含まれていたら\n",
    "                bow[col_i] = 1\n",
    "\n",
    "        if word in pn_table.keys(): # 1つのユーザ発話内の単語の極性スコアを加算\n",
    "            word_pn_scores += pn_table[word]\n",
    "    else:\n",
    "        # 1発話の単語について全て調べ終わったら、発話文の各単語の極性スコアの平均値を算出する\n",
    "        # 小数点以下切り捨て防止のため分子にfloat()、0除算防止のため分母に+1\n",
    "        word_pn_ave = float(word_pn_scores) / (len(words_in_inputted_talk) + 1)\n",
    "        bow.append(word_pn_ave)\n",
    "\n",
    "    bow = [bow] # 二次元である学習データに合わせるため\n",
    "    return bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "選択された場面：食べ残しをしないこと\n"
     ]
    }
   ],
   "source": [
    "scene_table = {1: \"掃除をすること\", 2: \"食べ残しをしないこと\", 3: \"早く寝ること\", 4: \"ゲームを止めること\", 5: \"適度な運動をとること\"}\n",
    "random_num = random.randint(1, 5) # ランダムに場面を選択する\n",
    "selected_scene = scene_table[random_num]\n",
    "request_sentence = \"\" # システムの依頼文\n",
    "\n",
    "if random_num == 1:\n",
    "    request_sentence = \"さあ、部屋を掃除しようよ。\"\n",
    "elif random_num == 2:\n",
    "    request_sentence = \"食べ残しは良くないよ...。\"\n",
    "elif random_num == 3:\n",
    "    request_sentence = \"早く寝ろよ。\"\n",
    "elif random_num == 4:\n",
    "    request_sentence = \"ゲームやりすぎじゃない？\"\n",
    "elif random_num == 5:\n",
    "    request_sentence = \"たまには運動しようぜ！\"\n",
    "print(\"選択された場面：{}\".format(selected_scene))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "system -> 食べ残しは良くないよ...。\n",
      "user -> 良いじゃん。\n",
      "ユーザ発話 = 良いじゃん。, 受諾度合いの予測値 = [ 2.27737739]\n",
      "[ 2.]\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    # システムから発話\n",
    "    print(\"system -> {}\".format(request_sentence))\n",
    "    \n",
    "    # ユーザの入力発話\n",
    "    inputted_usertalk = input(\"user -> \")\n",
    "    usertalk_bow = make_bow_of_inputted_usertalk(inputted_usertalk) # ユーザ発話を数値表現に変える\n",
    "    \n",
    "    # ユーザの受諾度合いの推定\n",
    "    user_acceptance = loaded_model.predict(usertalk_bow)\n",
    "    print(\"ユーザ発話 = {}, 受諾度合いの予測値 = {}\".format(inputted_usertalk, user_acceptance))\n",
    "    print(np.round(user_acceptance))\n",
    "    \n",
    "    # システム感情の遷移\n",
    "    \n",
    "    # 応答文選択\n",
    "    \n",
    "    # システムの応答\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
